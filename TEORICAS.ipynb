{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b7f552",
   "metadata": {},
   "source": [
    "# NOMBRE COMPLETO: DANIEL FELIPE QUINTERO JIMENEZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2b3b72",
   "metadata": {},
   "source": [
    "# Preguntas Te√≥ricas\n",
    "## 1. ¬øCu√°les son las diferencias fundamentales entre los modelos encoder-only, decoder-only y encoder-decoder en el contexto de los chatbots conversacionales? Explique qu√© tipo de modelo ser√≠a m√°s adecuado para cada caso de uso y por qu√©."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a93284",
   "metadata": {},
   "source": [
    "| Tipo de Modelo     | Caracter√≠sticas Principales                                                                 | Casos de Uso en Chatbots                                      | Ejemplos de Modelos     |\n",
    "|--------------------|---------------------------------------------------------------------------------------------|---------------------------------------------------------------|--------------------------|\n",
    "| **Encoder-only**   | Solo codifica texto de entrada. Bidireccional. No genera texto.                            | Clasificaci√≥n de intenciones, an√°lisis de sentimientos        | BERT, RoBERTa            |\n",
    "| **Decoder-only**   | Genera texto palabra por palabra. Autoregresivo. Unidireccional.                            | Generaci√≥n de respuestas, completado de texto                 | GPT, GPT-2, GPT-3        |\n",
    "| **Encoder-Decoder**| Codifica la entrada y luego genera una salida basada en esa codificaci√≥n.                  | Traducci√≥n, resumen, generaci√≥n controlada de respuestas      | T5, BART, mT5            |\n",
    "\n",
    "### ¬øQu√© tipo de modelo ser√≠a m√°s adecuado para cada caso de uso y por qu√©?\n",
    "\n",
    "- **Encoder-only**: √ötil para comprender lo que dice el usuario, como en la clasificaci√≥n de intenciones o detecci√≥n de entidades. No sirve para generar respuestas por s√≠ solo.\n",
    "- **Decoder-only**: Ideal para generar texto fluido y coherente, como respuestas en un chatbot. Usado cuando la prioridad es la generaci√≥n del lenguaje.\n",
    "- **Encoder-Decoder**: √ötil cuando se necesita transformar texto de una forma a otra, como en chatbots que resumen informaci√≥n, traducen, o responden con base en entradas complejas.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6bab77",
   "metadata": {},
   "source": [
    "## 2. Explique el concepto de \"temperatura\" en la generaci√≥n de texto con LLMs. ¬øC√≥mo afecta al comportamiento del chatbot y qu√© consideraciones debemos tener al ajustar este par√°metro para diferentes aplicaciones?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea589cc",
   "metadata": {},
   "source": [
    "## ¬øQu√© es la Temperatura en la Generaci√≥n de Texto con LLMs?\n",
    "\n",
    "La **temperatura** es un hiperpar√°metro que se utiliza durante la generaci√≥n de texto con modelos de lenguaje como GPT para controlar la **aleatoriedad** de las predicciones.\n",
    "\n",
    "### ¬øC√≥mo Funciona?\n",
    "\n",
    "Durante la generaci√≥n, el modelo calcula una distribuci√≥n de probabilidad sobre el siguiente token (palabra o subpalabra). La temperatura modifica esta distribuci√≥n:\n",
    "\n",
    "- Se aplica una f√≥rmula como esta:  \n",
    "P_i = exp(log(P_i) / T) / sum_j(exp(log(P_j) / T))\n",
    "\n",
    "Donde `T` es la temperatura.\n",
    "\n",
    "### Efecto de la Temperatura\n",
    "\n",
    "- **Temperatura = 1.0** (valor por defecto):  \n",
    "Comportamiento **est√°ndar** del modelo.\n",
    "\n",
    "- **Temperatura < 1.0** (por ejemplo, 0.2 a 0.7):  \n",
    "La distribuci√≥n se **agudiza** ‚Üí el modelo es **m√°s conservador y repetitivo**.  \n",
    "√ötil cuando se necesita **respuestas precisas, coherentes y controladas**.\n",
    "\n",
    "- **Temperatura > 1.0** (por ejemplo, 1.2 o m√°s):  \n",
    "La distribuci√≥n se **aplana** ‚Üí el modelo es **m√°s creativo, impredecible y variado**, pero puede volverse incoherente.  \n",
    "√ötil para tareas como escritura creativa o brainstorming.\n",
    "\n",
    "### Ejemplos de Aplicaci√≥n\n",
    "\n",
    "| Aplicaci√≥n                     | Temperatura Recomendada | Justificaci√≥n                                               |\n",
    "|-------------------------------|--------------------------|-------------------------------------------------------------|\n",
    "| Chatbot para atenci√≥n m√©dica  | 0.3 - 0.5                | Respuestas seguras, formales, coherentes                    |\n",
    "| Asistente educativo            | 0.5 - 0.7                | Cierta flexibilidad, pero con precisi√≥n en las explicaciones|\n",
    "| Generador de historias        | 1.0 - 1.3                | Mayor creatividad y diversidad de narrativas                |\n",
    "| Chat informal o roleplay      | 0.8 - 1.2                | Conversaciones variadas, espont√°neas y menos r√≠gidas        |\n",
    "\n",
    "### Consideraciones al Ajustar la Temperatura\n",
    "\n",
    "- No existe un valor √∫nico ideal: **depende del contexto de uso**.\n",
    "- Temperaturas muy altas pueden generar incoherencias o errores.\n",
    "- Se puede combinar con otros par√°metros como `top_k` o `top_p` para refinar a√∫n m√°s el control del texto generado.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusi√≥n:**  \n",
    "La temperatura es clave para modular el comportamiento del chatbot. Ajustarla correctamente puede marcar la diferencia entre una respuesta aburrida y repetitiva o una conversaci√≥n natural y efectiva.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14481232",
   "metadata": {},
   "source": [
    "## 3. Describa las t√©cnicas principales para reducir el problema de \"alucinaciones\" en chatbots basados en LLMs. ¬øQu√© estrategias podemos implementar a nivel de inferencia y a nivel de prompt engineering para mejorar la precisi√≥n factual de las respuestas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704466d4",
   "metadata": {},
   "source": [
    "## Reducci√≥n de Alucinaciones en Chatbots Basados en LLMs\n",
    "\n",
    "Las **alucinaciones** son respuestas generadas por modelos de lenguaje que parecen correctas pero que son **f√°cticamente incorrectas o inventadas**. Este es uno de los desaf√≠os principales en el uso de LLMs en aplicaciones cr√≠ticas como educaci√≥n, medicina o derecho.\n",
    "\n",
    "---\n",
    "\n",
    "### T√©cnicas para Reducir Alucinaciones\n",
    "\n",
    "#### üß† A. A Nivel de Inferencia\n",
    "\n",
    "1. **Temperatura baja**\n",
    "   - Usar valores de temperatura entre `0.2` y `0.5` reduce la creatividad del modelo, promoviendo respuestas m√°s seguras y precisas.\n",
    "\n",
    "2. **Restricci√≥n de tokens (top-k / top-p sampling)**\n",
    "   - Limitar la selecci√≥n de tokens a los m√°s probables (`top_k`) o acumulando una probabilidad l√≠mite (`top_p`) ayuda a evitar resultados improbables o aleatorios.\n",
    "\n",
    "3. **Consulta iterativa y verificaci√≥n**\n",
    "   - Generar respuestas en m√∫ltiples pasos o pedir al modelo que **verifique su respuesta** antes de responder definitivamente.\n",
    "\n",
    "4. **Recuperaci√≥n basada en documentos (RAG)**\n",
    "   - Combinar el LLM con un sistema de recuperaci√≥n de documentos externos (como una base de datos o motor de b√∫squeda) para responder con informaci√≥n verificada.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úçÔ∏è B. A Nivel de Prompt Engineering\n",
    "\n",
    "1. **Instrucciones claras y espec√≠ficas**\n",
    "   - Instrucciones como _‚ÄúResponde solo si est√°s seguro‚Äù_ o _‚ÄúSi no sabes la respuesta, responde ‚ÄòNo lo s√©‚Äô‚Äù_ pueden reducir invenciones.\n",
    "\n",
    "2. **Few-shot prompting con ejemplos reales**\n",
    "   - Incluir ejemplos correctos y bien estructurados en el prompt para establecer un patr√≥n deseado.\n",
    "\n",
    "3. **Uso de cadenas de pensamiento (\"chain-of-thought\")**\n",
    "   - Pedir al modelo que **razone paso a paso** puede mejorar la precisi√≥n l√≥gica y reducir errores f√°cticos.\n",
    "\n",
    "4. **Reforzar con roles o contexto de confiabilidad**\n",
    "   - Frases como _‚ÄúEres un experto en medicina. Responde con hechos basados en evidencia cient√≠fica‚Äù_ pueden guiar al modelo hacia mayor rigor.\n",
    "\n",
    "5. **Incorporaci√≥n de fuentes o referencias**\n",
    "   - Pedir al modelo que cite una fuente o diga de d√≥nde proviene la informaci√≥n:  \n",
    "     _‚ÄúIncluye la fuente de cada dato que menciones‚Äù_.\n",
    "\n",
    "---\n",
    "\n",
    "### Ejemplo de Prompt Mejorado\n",
    "\n",
    "```plaintext\n",
    "Eres un asistente legal que solo responde con informaci√≥n verificada. Si no tienes certeza, responde: \"No tengo suficiente informaci√≥n para responder con precisi√≥n\".\n",
    "\n",
    "Pregunta: ¬øCu√°l es la edad m√≠nima para votar en Argentina?\n",
    "\n",
    "Conclusi√≥n\n",
    "Reducir las alucinaciones en LLMs requiere una combinaci√≥n de ajustes t√©cnicos e ingenier√≠a de prompts. No existe una soluci√≥n √∫nica, pero aplicar varias de estas estrategias en conjunto puede mejorar considerablemente la precisi√≥n factual del chatbot.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
