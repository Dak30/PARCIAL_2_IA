{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3befa6d",
   "metadata": {},
   "source": [
    "PARCIAL 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5fd6b7",
   "metadata": {},
   "source": [
    "Ejercicio 1: Configuración del Entorno y Carga de Modelo Base\n",
    "Objetivo\n",
    "Establecer el entorno de desarrollo necesario para trabajar con modelos LLM y cargar un modelo pre-entrenado utilizando las bibliotecas Transformers y PyTorch.\n",
    "\n",
    "Código Base para Completar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbea068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea464874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Caché de modelos configurada en: C:\\Users\\Dani\\cache_modelos_hf\n",
      "\n",
      "=== Configuración Inicial ===\n",
      "\n",
      "[+] Cargando modelo: gpt2\n",
      "Intentando cargar con pesos de TensorFlow...\n",
      "\n",
      "Error al cargar el modelo: gpt2 does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.\n",
      "\n",
      "Probando con una versión alternativa del modelo...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6db70e055bd488898835649bb2090fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1d20421591461893b75d395050b9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e13f842f244f269ff350ec4126aed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8671e5bdcb174a568db41bb735a25f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994da7a5005a4fd7b0841fc22656abed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intentando cargar con pesos de TensorFlow...\n",
      "\n",
      "Error al cargar el modelo: distilgpt2 does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.\n"
     ]
    }
   ],
   "source": [
    "# Configurar y crear automáticamente la caché de modelos\n",
    "cache_dir = os.path.join(os.path.expanduser('~'), 'cache_modelos_hf')\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "Path(cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"\\nCaché de modelos configurada en: {cache_dir}\")\n",
    "\n",
    "def cargar_modelo(nombre_modelo):\n",
    "    \"\"\"\n",
    "    Carga un modelo pre-entrenado y su tokenizador correspondiente.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Configurar tokenizador\n",
    "        tokenizador = AutoTokenizer.from_pretrained(nombre_modelo, cache_dir=cache_dir)\n",
    "        tokenizador.pad_token = tokenizador.eos_token\n",
    "        \n",
    "        # Cargar modelo con manejo de formato TensorFlow/PyTorch\n",
    "        try:\n",
    "            modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo, cache_dir=cache_dir)\n",
    "        except:\n",
    "            print(\"Intentando cargar con pesos de TensorFlow...\")\n",
    "            modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo, cache_dir=cache_dir, from_tf=True)\n",
    "        \n",
    "        modelo.eval()\n",
    "        \n",
    "        # Optimizar para GPU\n",
    "        if torch.cuda.is_available():\n",
    "            modelo = modelo.half().to('cuda')\n",
    "            \n",
    "        return modelo, tokenizador\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError al cargar el modelo: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def verificar_dispositivo():\n",
    "    \"\"\"Verifica y muestra información del dispositivo disponible.\"\"\"\n",
    "    dispositivo = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    if dispositivo.type == 'cuda':\n",
    "        print(\"\\n[+] GPU Detectada:\")\n",
    "        print(f\"  - Nombre: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"  - Memoria: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f} GB\")\n",
    "    \n",
    "    return dispositivo\n",
    "\n",
    "def main():\n",
    "    print(\"\\n=== Configuración Inicial ===\")\n",
    "    dispositivo = verificar_dispositivo()\n",
    "    \n",
    "    # Seleccionar modelo según hardware\n",
    "    modelo_nombre = \"gpt2\"\n",
    "    print(f\"\\n[+] Cargando modelo: {modelo_nombre}\")\n",
    "    \n",
    "    modelo, tokenizador = cargar_modelo(modelo_nombre)\n",
    "    if modelo is None:\n",
    "        print(\"\\nProbando con una versión alternativa del modelo...\")\n",
    "        modelo_nombre = \"distilgpt2\"  # Versión más ligera alternativa\n",
    "        modelo, tokenizador = cargar_modelo(modelo_nombre)\n",
    "        if modelo is None:\n",
    "            return\n",
    "\n",
    "    # Configuración de generación\n",
    "    config_gen = {\n",
    "        'max_new_tokens': 100,\n",
    "        'do_sample': True,\n",
    "        'temperature': 0.7,\n",
    "        'top_p': 0.9,\n",
    "        'repetition_penalty': 1.2,\n",
    "        'pad_token_id': tokenizador.eos_token_id\n",
    "    }\n",
    "\n",
    "    print(\"\\nSistema listo. Escribe tu mensaje (o 'salir' para terminar)\")\n",
    "    while True:\n",
    "        try:\n",
    "            prompt = input(\"\\n[Tu mensaje]: \").strip()\n",
    "            if prompt.lower() in ['salir', 'exit', 'quit']:\n",
    "                break\n",
    "                \n",
    "            inputs = tokenizador(prompt, return_tensors='pt').to(dispositivo)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                salida = modelo.generate(**inputs, **config_gen)\n",
    "                \n",
    "            respuesta = tokenizador.decode(salida[0], skip_special_tokens=True)\n",
    "            print(f\"\\n[Respuesta]: {respuesta[len(prompt):].lstrip()}\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nOperación cancelada\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484c7c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La caché de modelos se guardará en: ./cache_modelos\n",
      "No se encontró GPU disponible, utilizando CPU.\n",
      "Utilizando dispositivo: cpu\n",
      "Cargando modelo: gpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo gpt2 y tokenizador cargados exitosamente.\n",
      "Utilizando CPU para la inferencia.\n",
      "Pregunta: Hola, ¿cómo estás?\n",
      "Respuesta generada: Hola, ¿cómo estás?\n",
      "\n",
      "I'm sorry, I'm sorry.\n",
      "\n",
      "I'm sorry, I'm sorry.\n",
      "\n",
      "I'm sorry, I'm sorry.\n",
      "\n",
      "I'm sorry, I'm sorry\n"
     ]
    }
   ],
   "source": [
    "# 2 INTENTOS\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# TODO: Configurar las variables de entorno para la caché de modelos\n",
    "# Establecer la carpeta donde se almacenarán los modelos descargados\n",
    "os.environ['TRANSFORMERS_CACHE'] = './cache_modelos'\n",
    "print(f\"La caché de modelos se guardará en: {os.environ['TRANSFORMERS_CACHE']}\")\n",
    "\n",
    "def cargar_modelo(nombre_modelo):\n",
    "    \"\"\"\n",
    "    Carga un modelo pre-entrenado y su tokenizador correspondiente.\n",
    "\n",
    "    Args:\n",
    "        nombre_modelo (str): Identificador del modelo en Hugging Face Hub\n",
    "    Returns:\n",
    "        tuple: (modelo, tokenizador)\n",
    "    \"\"\"\n",
    "    print(f\"Cargando modelo: {nombre_modelo}...\")\n",
    "    # TODO: Implementar la carga del modelo y tokenizador\n",
    "    # Utiliza AutoModelForCausalLM y AutoTokenizer\n",
    "    try:\n",
    "        modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo)\n",
    "        tokenizador = AutoTokenizer.from_pretrained(nombre_modelo)\n",
    "        print(f\"Modelo {nombre_modelo} y tokenizador cargados exitosamente.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el modelo {nombre_modelo}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # TODO: Configurar el modelo para inferencia (evaluar y usar half-precision si es posible)\n",
    "    modelo.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        modelo = modelo.half().cuda()\n",
    "        print(\"Modelo movido a GPU y utilizando half-precision (float16).\")\n",
    "    else:\n",
    "        print(\"Utilizando CPU para la inferencia.\")\n",
    "\n",
    "    return modelo, tokenizador\n",
    "\n",
    "def verificar_dispositivo():\n",
    "    \"\"\"\n",
    "    Verifica el dispositivo disponible (CPU/GPU) y muestra información relevante.\n",
    "\n",
    "    Returns:\n",
    "        torch.device: Dispositivo a utilizar\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        dispositivo = torch.device(\"cuda\")\n",
    "        # TODO: Si hay GPU disponible, mostrar información sobre la misma\n",
    "        print(f\"GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Cantidad de GPUs disponibles: {torch.cuda.device_count()}\")\n",
    "    else:\n",
    "        dispositivo = torch.device(\"cpu\")\n",
    "        print(\"No se encontró GPU disponible, utilizando CPU.\")\n",
    "    return dispositivo\n",
    "\n",
    "# Función principal de prueba\n",
    "def main():\n",
    "    dispositivo = verificar_dispositivo()\n",
    "    print(f\"Utilizando dispositivo: {dispositivo}\")\n",
    "\n",
    "    # TODO: Cargar un modelo pequeño adecuado para chatbots (ej. Mistral-7B, GPT2, etc.)\n",
    "    nombre_modelo = \"gpt2\"  # Un modelo pequeño para pruebas\n",
    "    modelo, tokenizador = cargar_modelo(nombre_modelo)\n",
    "\n",
    "    if modelo is not None and tokenizador is not None:\n",
    "        # TODO: Realizar una prueba simple de generación de texto\n",
    "        texto_prueba = \"Hola, ¿cómo estás?\"\n",
    "        input_ids = tokenizador.encode(texto_prueba, return_tensors=\"pt\").to(dispositivo)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = modelo.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "        respuesta = tokenizador.decode(output[0], skip_special_tokens=True)\n",
    "        print(f\"Pregunta: {texto_prueba}\")\n",
    "        print(f\"Respuesta generada: {respuesta}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a722f7",
   "metadata": {},
   "source": [
    "Ejercicio 2: Procesamiento de Entrada y Generación de Respuestas\n",
    "\n",
    "Objetivo: Desarrollar las funciones necesarias para procesar la entrada del usuario, preparar los tokens para el modelo y generar respuestas coherentes.\n",
    "\n",
    "Código Base para Completar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c550388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La caché de modelos se guardará en: ./cache_modelos\n",
      "No se encontró GPU disponible, utilizando CPU.\n",
      "Utilizando dispositivo: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo gpt2 y tokenizador cargados exitosamente.\n",
      "Utilizando CPU para la inferencia.\n",
      "Pregunta: Hola, ¿cómo estás?\n",
      "Respuesta generada: Hola, ¿cómo estás? O usted toda?\n",
      "\n",
      "(Cómo, á vías.)\n",
      "\n",
      "O, where are you?\n",
      "\n",
      "(Cómo, á vías\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# TODO: Configurar las variables de entorno para la caché de modelos\n",
    "# Establecer la carpeta donde se almacenarán los modelos descargados\n",
    "os.environ['TRANSFORMERS_CACHE'] = './cache_modelos'\n",
    "print(f\"La caché de modelos se guardará en: {os.environ['TRANSFORMERS_CACHE']}\")\n",
    "\n",
    "def cargar_modelo(nombre_modelo):\n",
    "    \"\"\"\n",
    "    Carga un modelo pre-entrenado y su tokenizador correspondiente.\n",
    "\n",
    "    Args:\n",
    "        nombre_modelo (str): Identificador del modelo en Hugging Face Hub\n",
    "\n",
    "    Returns:\n",
    "        tuple: (modelo, tokenizador)\n",
    "    \"\"\"\n",
    "    # TODO: Implementar la carga del modelo y tokenizador\n",
    "    # Utiliza AutoModelForCausalLM y AutoTokenizer\n",
    "    try:\n",
    "        modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo)\n",
    "        tokenizador = AutoTokenizer.from_pretrained(nombre_modelo)\n",
    "        print(f\"Modelo {nombre_modelo} y tokenizador cargados exitosamente.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el modelo {nombre_modelo}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # TODO: Configurar el modelo para inferencia (evaluar y usar half-precision si es posible)\n",
    "    modelo.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        modelo = modelo.half().cuda()\n",
    "        print(\"Modelo movido a GPU y utilizando half-precision (float16).\")\n",
    "    else:\n",
    "        print(\"Utilizando CPU para la inferencia.\")\n",
    "\n",
    "    return modelo, tokenizador\n",
    "\n",
    "def verificar_dispositivo():\n",
    "    \"\"\"\n",
    "    Verifica el dispositivo disponible (CPU/GPU) y muestra información relevante.\n",
    "\n",
    "    Returns:\n",
    "        torch.device: Dispositivo a utilizar\n",
    "    \"\"\"\n",
    "    # TODO: Implementar la detección del dispositivo\n",
    "    if torch.cuda.is_available():\n",
    "        dispositivo = torch.device(\"cuda\")\n",
    "        # TODO: Si hay GPU disponible, mostrar información sobre la misma\n",
    "        print(f\"GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Cantidad de GPUs disponibles: {torch.cuda.device_count()}\")\n",
    "    else:\n",
    "        dispositivo = torch.device(\"cpu\")\n",
    "        print(\"No se encontró GPU disponible, utilizando CPU.\")\n",
    "    return dispositivo\n",
    "\n",
    "# Función principal de prueba\n",
    "def main():\n",
    "    dispositivo = verificar_dispositivo()\n",
    "    print(f\"Utilizando dispositivo: {dispositivo}\")\n",
    "\n",
    "    # TODO: Cargar un modelo pequeño adecuado para chatbots (ej. Mistral-7B, GPT2, etc.)\n",
    "    nombre_modelo = \"gpt2\"  # Un modelo pequeño para pruebas\n",
    "    modelo, tokenizador = cargar_modelo(nombre_modelo)\n",
    "\n",
    "    if modelo is not None and tokenizador is not None:\n",
    "        # TODO: Realizar una prueba simple de generación de texto\n",
    "        texto_prueba = \"Hola, ¿cómo estás?\"\n",
    "        entrada_tokenizada = tokenizador(texto_prueba, return_tensors=\"pt\")\n",
    "        input_ids = entrada_tokenizada.input_ids.to(dispositivo)\n",
    "        attention_mask = entrada_tokenizada.attention_mask.to(dispositivo)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = modelo.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=50,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.8,\n",
    "                top_p=0.9,\n",
    "                do_sample=True  # Habilitar el muestreo para usar temperature y top_p\n",
    "            )\n",
    "\n",
    "        respuesta = tokenizador.decode(output[0], skip_special_tokens=True)\n",
    "        print(f\"Pregunta: {texto_prueba}\")\n",
    "        print(f\"Respuesta generada: {respuesta}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c09398",
   "metadata": {},
   "source": [
    "Ejercicio 3: Manejo de Contexto Conversacional\n",
    "\n",
    "Objetivo:\n",
    "Implementar un sistema para mantener el contexto de la conversación, permitiendo al chatbot recordar intercambios anteriores y responder coherentemente a conversaciones prolongadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7176c70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot inicializado con el modelo: gpt2\n",
      "Instrucciones del sistema: Eres un asistente amigable y servicial.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "def cargar_modelo(modelo_id):\n",
    "    \"\"\"Carga el modelo y el tokenizador desde Hugging Face.\"\"\"\n",
    "    tokenizador = AutoTokenizer.from_pretrained(modelo_id)\n",
    "    modelo = AutoModelForCausalLM.from_pretrained(modelo_id)\n",
    "    return modelo, tokenizador\n",
    "\n",
    "def verificar_dispositivo():\n",
    "    \"\"\"Verifica y devuelve el dispositivo a usar (CPU o GPU).\"\"\"\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class GestorContexto:\n",
    "    \"\"\"\n",
    "    Clase para gestionar el contexto de una conversación con el chatbot.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, longitud_maxima=1024, formato_mensaje=None):\n",
    "        \"\"\"\n",
    "        Inicializa el gestor de contexto.\n",
    "\n",
    "        Args:\n",
    "            longitud_maxima (int): Número máximo de tokens a mantener en el contexto\n",
    "            formato_mensaje (callable): Función para formatear mensajes (por defecto, None)\n",
    "        \"\"\"\n",
    "        self.historial = []\n",
    "        self.longitud_maxima = longitud_maxima\n",
    "        self.formato_mensaje = formato_mensaje or self._formato_predeterminado\n",
    "\n",
    "    def _formato_predeterminado(self, rol, contenido):\n",
    "        \"\"\"\n",
    "        Formato predeterminado para mensajes.\n",
    "\n",
    "        Args:\n",
    "            rol (str): 'sistema', 'usuario' o 'asistente'\n",
    "            contenido (str): Contenido del mensaje\n",
    "\n",
    "        Returns:\n",
    "            str: Mensaje formateado\n",
    "        \"\"\"\n",
    "        if rol == \"sistema\":\n",
    "            return f\"<<SYSTEM>> {contenido} <</SYSTEM>>\"\n",
    "        elif rol == \"usuario\":\n",
    "            return f\"<<USER>> {contenido} <</USER>>\"\n",
    "        elif rol == \"asistente\":\n",
    "            return f\"<<ASSISTANT>> {contenido} <</ASSISTANT>>\"\n",
    "        return f\"{rol}: {contenido}\"\n",
    "\n",
    "    def agregar_mensaje(self, rol, contenido):\n",
    "        \"\"\"\n",
    "        Agrega un mensaje al historial de conversación.\n",
    "\n",
    "        Args:\n",
    "            rol (str): 'sistema', 'usuario' o 'asistente'\n",
    "            contenido (str): Contenido del mensaje\n",
    "        \"\"\"\n",
    "        self.historial.append({\"rol\": rol, \"contenido\": contenido})\n",
    "\n",
    "    def construir_prompt_completo(self):\n",
    "        \"\"\"\n",
    "        Construye un prompt completo basado en el historial.\n",
    "\n",
    "        Returns:\n",
    "            str: Prompt completo para el modelo\n",
    "        \"\"\"\n",
    "        mensajes_formateados = [self.formato_mensaje(msg[\"rol\"], msg[\"contenido\"]) for msg in self.historial]\n",
    "        return \"\\n\".join(mensajes_formateados)\n",
    "\n",
    "    def truncar_historial(self, tokenizador):\n",
    "        \"\"\"\n",
    "        Trunca el historial si excede la longitud máxima.\n",
    "\n",
    "        Args:\n",
    "            tokenizador: Tokenizador del modelo\n",
    "        \"\"\"\n",
    "        total_tokens = 0\n",
    "        indices_a_eliminar = []\n",
    "\n",
    "        # Calcular el número total de tokens en el historial\n",
    "        for i, mensaje in enumerate(reversed(self.historial)):\n",
    "            tokens = tokenizador.encode(self.formato_mensaje(mensaje[\"rol\"], mensaje[\"contenido\"]), add_special_tokens=False)\n",
    "            total_tokens += len(tokens)\n",
    "            if total_tokens > self.longitud_maxima:\n",
    "                indices_a_eliminar.append(len(self.historial) - 1 - i)\n",
    "\n",
    "        # Eliminar los mensajes más antiguos si se excede la longitud máxima\n",
    "        if indices_a_eliminar:\n",
    "            indices_a_eliminar.reverse()  # Eliminar en orden ascendente para no afectar los índices\n",
    "            nuevo_historial = [msg for i, msg in enumerate(self.historial) if i not in indices_a_eliminar]\n",
    "            self.historial = nuevo_historial\n",
    "\n",
    "        # Consideración adicional: Podrías implementar una lógica más compleja aquí,\n",
    "        # como resumir mensajes antiguos en lugar de simplemente eliminarlos,\n",
    "        # o asegurarte de mantener siempre el mensaje del sistema.\n",
    "\n",
    "class Chatbot:\n",
    "    \"\"\"\n",
    "    Implementación de chatbot con manejo de contexto.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, modelo_id, instrucciones_sistema=None):\n",
    "        \"\"\"\n",
    "        Inicializa el chatbot.\n",
    "\n",
    "        Args:\n",
    "            modelo_id (str): Identificador del modelo en Hugging Face\n",
    "            instrucciones_sistema (str): Instrucciones de comportamiento del sistema\n",
    "        \"\"\"\n",
    "        self.modelo, self.tokenizador = cargar_modelo(modelo_id)\n",
    "        self.dispositivo = verificar_dispositivo()\n",
    "        self.gestor_contexto = GestorContexto()\n",
    "\n",
    "        # Inicializar el contexto con instrucciones del sistema\n",
    "        if instrucciones_sistema:\n",
    "            self.gestor_contexto.agregar_mensaje(\"sistema\", instrucciones_sistema)\n",
    "\n",
    "    def responder(self, mensaje_usuario, parametros_generacion=None):\n",
    "        \"\"\"\n",
    "        Genera una respuesta al mensaje del usuario.\n",
    "\n",
    "        Args:\n",
    "            mensaje_usuario (str): Mensaje del usuario\n",
    "            parametros_generacion (dict): Parámetros para la generación\n",
    "\n",
    "        Returns:\n",
    "            str: Respuesta del chatbot\n",
    "        \"\"\"\n",
    "        # 1. Agregar mensaje del usuario al contexto\n",
    "        self.gestor_contexto.agregar_mensaje(\"usuario\", mensaje_usuario)\n",
    "\n",
    "        # 2. Construir el prompt completo\n",
    "        prompt = self.gestor_contexto.construir_prompt_completo()\n",
    "\n",
    "        # 3. Preprocesar la entrada\n",
    "        inputs = self.tokenizador(prompt, return_tensors=\"pt\").to(self.dispositivo)\n",
    "\n",
    "        # 4. Generar la respuesta\n",
    "        parametros_generacion = parametros_generacion if parametros_generacion else {\n",
    "            \"max_length\": 150,\n",
    "            \"num_beams\": 5,\n",
    "            \"no_repeat_ngram_size\": 2,\n",
    "            \"early_stopping\": True,\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            output = self.modelo.generate(**inputs, **parametros_generacion)\n",
    "\n",
    "        # 5. Decodificar la respuesta y agregarla al contexto\n",
    "        respuesta = self.tokenizador.decode(output[0], skip_special_tokens=True)\n",
    "        # Considerar cómo extraer solo la parte de la respuesta del asistente\n",
    "        # si el modelo genera el prompt completo de vuelta.\n",
    "        respuesta_limpia = respuesta.replace(prompt, \"\").strip()\n",
    "        self.gestor_contexto.agregar_mensaje(\"asistente\", respuesta_limpia)\n",
    "\n",
    "        # 6. Truncar el historial si es necesario\n",
    "        self.gestor_contexto.truncar_historial(self.tokenizador)\n",
    "\n",
    "        return respuesta_limpia\n",
    "\n",
    "def prueba_conversacion():\n",
    "    # Crear una instancia del chatbot\n",
    "    modelo_id = \"gpt2\"  # Puedes probar con un modelo más pequeño para empezar\n",
    "    instrucciones_sistema = \"Eres un asistente amigable y servicial.\"\n",
    "    chatbot = Chatbot(modelo_id, instrucciones_sistema)\n",
    "\n",
    "    print(f\"Chatbot inicializado con el modelo: {modelo_id}\")\n",
    "    print(f\"Instrucciones del sistema: {instrucciones_sistema}\\n\")\n",
    "\n",
    "    # Simular una conversación de varios turnos\n",
    "    while True:\n",
    "        mensaje_usuario = input(\"Usuario: \")\n",
    "        if mensaje_usuario.lower() == \"salir\":\n",
    "            break\n",
    "\n",
    "        respuesta = chatbot.responder(mensaje_usuario)\n",
    "        print(f\"Asistente: {respuesta}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prueba_conversacion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "870dd137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usuario: Hola, ¿cómo estás?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asistente: sistema: Este es un chatbot que recuerda los mensajes anteriores.\n",
      "usuario: Hola, ¿cómo estás?\n",
      "\n",
      "usuario: Hola, ¿cómo estás?\n",
      "\n",
      "usuario: Hola, ¿cómo estás?\n",
      "\n",
      "usuario: Hola, ¿cómo estás\n",
      "\n",
      "Usuario: ¿Qué puedes hacer?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asistente: sistema: Este es un chatbot que recuerda los mensajes anteriores.\n",
      "usuario: Hola, ¿cómo estás?\n",
      "asistente: sistema: Este es un chatbot que recuerda los mensajes anteriores.\n",
      "usuario: Hola, ¿cómo estás?\n",
      "\n",
      "usuario: Hola, ¿cómo estás?\n",
      "\n",
      "usuario: Hola, ¿cómo estás?\n",
      "\n",
      "usuario: Hola, ¿cómo estás\n",
      "usuario: ¿Qué puedes hacer?\n",
      "\n",
      "asistente: sistema: Este es un chatbot que recuerda los mensajes anteriores.\n",
      "\n",
      "usuario: Hola, ¿cómo estás?\n",
      "\n",
      "us\n",
      "\n",
      "Usuario: ¿Puedes recordar lo que hablamos antes?\n",
      "Asistente: sistema: Este es un chatbot que recuerda los mensajes anteriores.\n",
      "usuario: Hola, ¿cómo estás?\n",
      "asistente: sistema: Este es un chatbot que recuerda los mensajes anteriores.\n",
      "usuario: Hola, ¿cómo estás?\n",
      "\n",
      "usuario: Hola, ¿cómo estás?\n",
      "\n",
      "usuario: Hola, ¿cómo estás?\n",
      "\n",
      "usuario: Hola, ¿cómo estás\n",
      "usuario: ¿Qué puedes hacer?\n",
      "asistente: sistema: Este es un chatbot que recuerda los mensajes anteriores.\n",
      "usuario: Hola, ¿cómo estás?\n",
      "asistente: sistema: Este es un chatbot que recuerda los mensajes anteriores.\n",
      "usuario: Hola, ¿cómo estás?\n",
      "\n",
      "usuario: Hola, ¿cómo estás?\n",
      "\n",
      "usuario: Hola, ¿cómo estás?\n",
      "\n",
      "usuario: Hola, ¿cómo estás\n",
      "usuario: ¿Qué puedes hacer?\n",
      "\n",
      "asistente: sistema: Este es un chatbot que recuerda los mensajes anteriores.\n",
      "\n",
      "usuario: Hola, ¿cómo estás?\n",
      "\n",
      "us\n",
      "usuario: ¿Puedes recordar lo que hablamos antes?\n",
      "\n",
      "asistente: sistema: Este es un chatbot que recuerda los mensajes anteriores.\n",
      "\n",
      "usuario: Hola, ¿cómo estás?\n",
      "\n",
      "us\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "class GestorContexto:\n",
    "    \"\"\"\n",
    "    Clase para gestionar el contexto de una conversación con el chatbot.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, longitud_maxima=1024, formato_mensaje=None):\n",
    "        \"\"\"\n",
    "        Inicializa el gestor de contexto.\n",
    "        \n",
    "        Args:\n",
    "            longitud_maxima (int): Número máximo de tokens a mantener en el contexto.\n",
    "            formato_mensaje (callable): Función para formatear mensajes (por defecto, None).\n",
    "        \"\"\"\n",
    "        self.historial = []\n",
    "        self.longitud_maxima = longitud_maxima\n",
    "        self.formato_mensaje = formato_mensaje or self._formato_predeterminado\n",
    "        \n",
    "    def _formato_predeterminado(self, rol, contenido):\n",
    "        \"\"\"\n",
    "        Formato predeterminado para mensajes.\n",
    "        \n",
    "        Args:\n",
    "            rol (str): 'sistema', 'usuario' o 'asistente'.\n",
    "            contenido (str): Contenido del mensaje.\n",
    "            \n",
    "        Returns:\n",
    "            str: Mensaje formateado.\n",
    "        \"\"\"\n",
    "        return f\"{rol}: {contenido}\"\n",
    "    \n",
    "    def agregar_mensaje(self, rol, contenido):\n",
    "        \"\"\"\n",
    "        Agrega un mensaje al historial de conversación.\n",
    "        \n",
    "        Args:\n",
    "            rol (str): 'sistema', 'usuario' o 'asistente'.\n",
    "            contenido (str): Contenido del mensaje.\n",
    "        \"\"\"\n",
    "        mensaje = self.formato_mensaje(rol, contenido)\n",
    "        self.historial.append(mensaje)\n",
    "        self.truncar_historial()\n",
    "\n",
    "    def construir_prompt_completo(self):\n",
    "        \"\"\"\n",
    "        Construye un prompt completo basado en el historial.\n",
    "        \n",
    "        Returns:\n",
    "            str: Prompt completo para el modelo.\n",
    "        \"\"\"\n",
    "        return \"\\n\".join(self.historial)\n",
    "    \n",
    "    def truncar_historial(self):\n",
    "        \"\"\"\n",
    "        Trunca el historial si excede la longitud máxima.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            # Convertir el historial a tokens y verificar la longitud\n",
    "            tokens = self.historial  # Aquí se debería usar un tokenizador real para convertir el texto a tokens\n",
    "            if len(tokens) <= self.longitud_maxima:\n",
    "                break\n",
    "            # Eliminar el mensaje más antiguo\n",
    "            self.historial.pop(0)\n",
    "            \n",
    "class Chatbot:\n",
    "    \"\"\"\n",
    "    Implementación de chatbot con manejo de contexto.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, modelo_id, instrucciones_sistema=None):\n",
    "        \"\"\"\n",
    "        Inicializa el chatbot.\n",
    "        \n",
    "        Args:\n",
    "            modelo_id (str): Identificador del modelo en Hugging Face.\n",
    "            instrucciones_sistema (str): Instrucciones de comportamiento del sistema.\n",
    "        \"\"\"\n",
    "        self.tokenizador = GPT2Tokenizer.from_pretrained(modelo_id)\n",
    "        self.modelo = GPT2LMHeadModel.from_pretrained(modelo_id)\n",
    "        self.dispositivo = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.modelo.to(self.dispositivo)\n",
    "        \n",
    "        self.gestor_contexto = GestorContexto()\n",
    "        \n",
    "        # Inicializar el contexto con instrucciones del sistema, si las hay\n",
    "        if instrucciones_sistema:\n",
    "            self.gestor_contexto.agregar_mensaje('sistema', instrucciones_sistema)\n",
    "        \n",
    "        # Establecer el token de padding para evitar problemas\n",
    "        self.tokenizador.pad_token = self.tokenizador.eos_token\n",
    "    \n",
    "    def responder(self, mensaje_usuario, parametros_generacion=None):\n",
    "        \"\"\"\n",
    "        Genera una respuesta al mensaje del usuario.\n",
    "        \n",
    "        Args:\n",
    "            mensaje_usuario (str): Mensaje del usuario.\n",
    "            parametros_generacion (dict): Parámetros para la generación.\n",
    "            \n",
    "        Returns:\n",
    "            str: Respuesta del chatbot.\n",
    "        \"\"\"\n",
    "        # Evitar agregar el mensaje del sistema repetidamente\n",
    "        self.gestor_contexto.agregar_mensaje('usuario', mensaje_usuario)\n",
    "        \n",
    "        # Construir el prompt completo basado en el historial\n",
    "        prompt_completo = self.gestor_contexto.construir_prompt_completo()\n",
    "        \n",
    "        # Preprocesar la entrada\n",
    "        entrada_procesada = self.preprocesar_entrada(prompt_completo)\n",
    "        \n",
    "        # Generar la respuesta utilizando el modelo\n",
    "        respuesta = self.generar_respuesta(entrada_procesada, parametros_generacion)\n",
    "        \n",
    "        # Agregar respuesta del asistente al contexto\n",
    "        self.gestor_contexto.agregar_mensaje('asistente', respuesta)\n",
    "        \n",
    "        return respuesta\n",
    "    \n",
    "    def preprocesar_entrada(self, prompt_completo):\n",
    "        \"\"\"\n",
    "        Preprocesa la entrada antes de pasarlo al modelo.\n",
    "        \n",
    "        Args:\n",
    "            prompt_completo (str): El prompt completo construido.\n",
    "        \n",
    "        Returns:\n",
    "            str: Entrada preprocesada.\n",
    "        \"\"\"\n",
    "        return prompt_completo\n",
    "    \n",
    "    def generar_respuesta(self, entrada_procesada, parametros_generacion):\n",
    "        \"\"\"\n",
    "        Genera la respuesta utilizando el modelo.\n",
    "        \n",
    "        Args:\n",
    "            entrada_procesada (str): La entrada que será procesada por el modelo.\n",
    "            parametros_generacion (dict): Parámetros para la generación.\n",
    "        \n",
    "        Returns:\n",
    "            str: La respuesta generada por el modelo.\n",
    "        \"\"\"\n",
    "        input_ids = self.tokenizador.encode(entrada_procesada, return_tensors=\"pt\").to(self.dispositivo)\n",
    "        \n",
    "        # Crear la máscara de atención\n",
    "        attention_mask = (input_ids != self.tokenizador.pad_token_id).type(torch.uint8).to(self.dispositivo)\n",
    "        \n",
    "        # Ajustar el valor de max_new_tokens si la entrada es demasiado larga\n",
    "        max_new_tokens = 50  # Ajusta esto según sea necesario\n",
    "        \n",
    "        # Verificar si la longitud de la entrada excede el límite\n",
    "        if input_ids.shape[1] > 1024:  # Suponiendo que el modelo tiene un límite de 1024 tokens\n",
    "            input_ids = input_ids[:, -1024:]  # Recortar la entrada a los últimos 1024 tokens\n",
    "        \n",
    "        # Generación de la respuesta\n",
    "        outputs = self.modelo.generate(input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, num_return_sequences=1)\n",
    "        \n",
    "        respuesta = self.tokenizador.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return respuesta\n",
    "\n",
    "def prueba_conversacion():\n",
    "    # Crear una instancia del chatbot\n",
    "    chatbot = Chatbot(\"gpt2\", instrucciones_sistema=\"Este es un chatbot que recuerda los mensajes anteriores.\")\n",
    "    \n",
    "    # Simular una conversación de varios turnos\n",
    "    print(\"Usuario: Hola, ¿cómo estás?\")\n",
    "    respuesta = chatbot.responder(\"Hola, ¿cómo estás?\")\n",
    "    print(\"Asistente:\", respuesta)\n",
    "    \n",
    "    print(\"\\nUsuario: ¿Qué puedes hacer?\")\n",
    "    respuesta = chatbot.responder(\"¿Qué puedes hacer?\")\n",
    "    print(\"Asistente:\", respuesta)\n",
    "    \n",
    "    print(\"\\nUsuario: ¿Puedes recordar lo que hablamos antes?\")\n",
    "    respuesta = chatbot.responder(\"¿Puedes recordar lo que hablamos antes?\")\n",
    "    print(\"Asistente:\", respuesta)\n",
    "\n",
    "# Ejecutar la prueba de conversación\n",
    "prueba_conversacion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088c425f",
   "metadata": {},
   "source": [
    "Ejercicio 4: Optimización del Modelo para Recursos Limitados\n",
    "\n",
    "Objetivo:\n",
    "\n",
    "Implementar técnicas de optimización para mejorar la velocidad de inferencia y reducir el consumo de memoria, permitiendo que el chatbot funcione eficientemente en dispositivos con recursos limitados.\n",
    "\n",
    "Código Base para Completar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f944c05",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 145\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRendimiento del modelo con todas las optimizaciones:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rendimiento_optimizado)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# Ejecutar la demostración de optimizaciones\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m \u001b[43mdemo_optimizaciones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 126\u001b[0m, in \u001b[0;36mdemo_optimizaciones\u001b[1;34m()\u001b[0m\n\u001b[0;32m    123\u001b[0m rendimiento_base \u001b[38;5;241m=\u001b[39m evaluar_rendimiento(modelo_base, tokenizador_base, texto_prueba, dispositivo\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# 2. Cargar el modelo con cuantización de 4 bits\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m modelo_cuantizado_4, tokenizador_cuantizado_4 \u001b[38;5;241m=\u001b[39m \u001b[43mcargar_modelo_optimizado\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizaciones\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuantizacion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m rendimiento_cuantizado_4 \u001b[38;5;241m=\u001b[39m evaluar_rendimiento(modelo_cuantizado_4, tokenizador_cuantizado_4, texto_prueba, dispositivo\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# 3. Cargar el modelo con atención de ventana deslizante\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 43\u001b[0m, in \u001b[0;36mcargar_modelo_optimizado\u001b[1;34m(nombre_modelo, optimizaciones)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizaciones[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuantizacion\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m optimizaciones[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbits\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m---> 43\u001b[0m         modelo \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnombre_modelo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m optimizaciones[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbits\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m8\u001b[39m:\n\u001b[0;32m     49\u001b[0m         modelo \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     50\u001b[0m             nombre_modelo,\n\u001b[0;32m     51\u001b[0m             load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     52\u001b[0m             device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\modeling_utils.py:4139\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4137\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepSpeed Zero-3 is not compatible with passing a `device_map`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 4139\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   4140\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4141\u001b[0m         )\n\u001b[0;32m   4143\u001b[0m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[0;32m   4144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[1;31mValueError\u001b[0m: Using a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "def configurar_cuantizacion(bits=4):\n",
    "    \"\"\"\n",
    "    Configura los parámetros para la cuantización del modelo.\n",
    "    \n",
    "    Args:\n",
    "        bits (int): Bits para cuantización (4 u 8)\n",
    "    \n",
    "    Returns:\n",
    "        BitsAndBytesConfig: Configuración de cuantización\n",
    "    \"\"\"\n",
    "    if bits not in [4, 8]:\n",
    "        raise ValueError(\"Solo se permite cuantización de 4 o 8 bits\")\n",
    "    \n",
    "    # Configurar la cuantización con BitsAndBytesConfig\n",
    "    config_cuantizacion = BitsAndBytesConfig(\n",
    "        load_in_8bit=bits == 8,  # Usar 8 bits si 'bits' es 8\n",
    "        load_in_4bit=bits == 4   # Usar 4 bits si 'bits' es 4\n",
    "    )\n",
    "    \n",
    "    return config_cuantizacion\n",
    "\n",
    "def cargar_modelo_optimizado(nombre_modelo, optimizaciones=None):\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "    if optimizaciones is None:\n",
    "        optimizaciones = {\n",
    "            \"cuantizacion\": True,\n",
    "            \"bits\": 4,\n",
    "            \"offload_cpu\": False,\n",
    "            \"flash_attention\": False\n",
    "        }\n",
    "\n",
    "    tokenizador = AutoTokenizer.from_pretrained(nombre_modelo)\n",
    "\n",
    "    # Cargar modelo con cuantización\n",
    "    if optimizaciones[\"cuantizacion\"]:\n",
    "        if optimizaciones[\"bits\"] == 4:\n",
    "            modelo = AutoModelForCausalLM.from_pretrained(\n",
    "                nombre_modelo,\n",
    "                load_in_4bit=True,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        elif optimizaciones[\"bits\"] == 8:\n",
    "            modelo = AutoModelForCausalLM.from_pretrained(\n",
    "                nombre_modelo,\n",
    "                load_in_8bit=True,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Bits debe ser 4 u 8 para cuantización\")\n",
    "    else:\n",
    "        modelo = AutoModelForCausalLM.from_pretrained(nombre_modelo)\n",
    "\n",
    "    return modelo, tokenizador\n",
    "\n",
    "\n",
    "def aplicar_sliding_window(modelo, window_size=1024):\n",
    "    \"\"\"\n",
    "    Configura la atención de ventana deslizante para procesar secuencias largas.\n",
    "    \n",
    "    Args:\n",
    "        modelo: Modelo a configurar\n",
    "        window_size (int): Tamaño de la ventana de atención\n",
    "    \"\"\"\n",
    "    # Este paso depende de si el modelo soporta la técnica de \"sliding window\".\n",
    "    # Para algunos modelos, como Longformer, se puede aplicar esta técnica.\n",
    "    # En GPT2, este tipo de atención no está disponible nativamente.\n",
    "    pass\n",
    "\n",
    "def evaluar_rendimiento(modelo, tokenizador, texto_prueba, dispositivo):\n",
    "    \"\"\"\n",
    "    Evalúa el rendimiento del modelo en términos de velocidad y memoria.\n",
    "    \n",
    "    Args:\n",
    "        modelo: Modelo a evaluar\n",
    "        tokenizador: Tokenizador del modelo\n",
    "        texto_prueba (str): Texto para pruebas de rendimiento\n",
    "        dispositivo: Dispositivo donde se ejecutará\n",
    "    \n",
    "    Returns:\n",
    "        dict: Métricas de rendimiento\n",
    "    \"\"\"\n",
    "    # Medir tiempo de inferencia\n",
    "    start_time = time.time()\n",
    "    inputs = tokenizador(texto_prueba, return_tensors=\"pt\").to(dispositivo)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        modelo.eval()\n",
    "        outputs = modelo(**inputs)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calcular el uso de memoria\n",
    "    memoria_inicial = psutil.virtual_memory().used\n",
    "    memoria_final = psutil.virtual_memory().used\n",
    "    uso_memoria = memoria_final - memoria_inicial  # en bytes\n",
    "    \n",
    "    # Calcular tokens por segundo\n",
    "    tiempo_inferencia = end_time - start_time\n",
    "    tokens = len(inputs[\"input_ids\"][0])\n",
    "    tokens_por_segundo = tokens / tiempo_inferencia if tiempo_inferencia > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"tiempo_inferencia\": tiempo_inferencia,\n",
    "        \"uso_memoria\": uso_memoria,\n",
    "        \"tokens_por_segundo\": tokens_por_segundo\n",
    "    }\n",
    "\n",
    "def demo_optimizaciones():\n",
    "    \"\"\"\n",
    "    Demuestra el impacto de diferentes optimizaciones en el rendimiento.\n",
    "    \"\"\"\n",
    "    # Texto de prueba\n",
    "    texto_prueba = \"Este es un texto de prueba para evaluar el rendimiento del modelo.\"\n",
    "\n",
    "    # 1. Cargar el modelo base (sin optimizaciones)\n",
    "    modelo_base, tokenizador_base = cargar_modelo_optimizado(\"gpt2\", optimizaciones={\"cuantizacion\": False})\n",
    "    rendimiento_base = evaluar_rendimiento(modelo_base, tokenizador_base, texto_prueba, dispositivo=\"cpu\")\n",
    "    \n",
    "    # 2. Cargar el modelo con cuantización de 4 bits\n",
    "    modelo_cuantizado_4, tokenizador_cuantizado_4 = cargar_modelo_optimizado(\"gpt2\", optimizaciones={\"cuantizacion\": True, \"bits\": 4})\n",
    "    rendimiento_cuantizado_4 = evaluar_rendimiento(modelo_cuantizado_4, tokenizador_cuantizado_4, texto_prueba, dispositivo=\"cpu\")\n",
    "    \n",
    "    # 3. Cargar el modelo con atención de ventana deslizante\n",
    "    modelo_con_sliding_window, tokenizador_sliding_window = cargar_modelo_optimizado(\"gpt2\", optimizaciones={\"cuantizacion\": False})\n",
    "    aplicar_sliding_window(modelo_con_sliding_window, window_size=1024)\n",
    "    rendimiento_sliding_window = evaluar_rendimiento(modelo_con_sliding_window, tokenizador_sliding_window, texto_prueba, dispositivo=\"cpu\")\n",
    "    \n",
    "    # 4. Cargar el modelo con todas las optimizaciones\n",
    "    modelo_optimizado, tokenizador_optimizado = cargar_modelo_optimizado(\"gpt2\", optimizaciones={\"cuantizacion\": True, \"bits\": 4, \"offload_cpu\": False, \"flash_attention\": True})\n",
    "    rendimiento_optimizado = evaluar_rendimiento(modelo_optimizado, tokenizador_optimizado, texto_prueba, dispositivo=\"cpu\")\n",
    "    \n",
    "    # Mostrar los resultados\n",
    "    print(\"Rendimiento del modelo base:\", rendimiento_base)\n",
    "    print(\"Rendimiento del modelo con cuantización de 4 bits:\", rendimiento_cuantizado_4)\n",
    "    print(\"Rendimiento del modelo con sliding window:\", rendimiento_sliding_window)\n",
    "    print(\"Rendimiento del modelo con todas las optimizaciones:\", rendimiento_optimizado)\n",
    "\n",
    "# Ejecutar la demostración de optimizaciones\n",
    "demo_optimizaciones()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6efb9f6",
   "metadata": {},
   "source": [
    "Ejercicio 5: Personalización del Chatbot y Despliegue\n",
    "\n",
    "Objetivo:\n",
    "Implementar técnicas para personalizar el comportamiento del chatbot y prepararlo para su despliegue como una aplicación web simple.\n",
    "\n",
    "Código Base para Completar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "def configurar_peft(modelo, r=8, lora_alpha=32):\n",
    "    \"\"\"\n",
    "    Configura el modelo para fine-tuning con PEFT/LoRA.\n",
    "    \n",
    "    Args:\n",
    "        modelo: Modelo base\n",
    "        r (int): Rango de adaptadores LoRA\n",
    "        lora_alpha (int): Escala alpha para LoRA\n",
    "    \n",
    "    Returns:\n",
    "        modelo: Modelo adaptado para fine-tuning\n",
    "    \"\"\"\n",
    "    # TODO: Implementar la configuración de PEFT\n",
    "    # Crear LoraConfig y aplicarla al modelo\n",
    "    # ...\n",
    "    \n",
    "    return modelo_peft\n",
    "\n",
    "def guardar_modelo(modelo, tokenizador, ruta):\n",
    "    \"\"\"\n",
    "    Guarda el modelo y tokenizador en una ruta específica.\n",
    "    \n",
    "    Args:\n",
    "        modelo: Modelo a guardar\n",
    "        tokenizador: Tokenizador del modelo\n",
    "        ruta (str): Ruta donde guardar\n",
    "    \"\"\"\n",
    "    # TODO: Implementar el guardado de modelo y tokenizador\n",
    "    # ...\n",
    "\n",
    "def cargar_modelo_personalizado(ruta):\n",
    "    \"\"\"\n",
    "    Carga un modelo personalizado desde una ruta específica.\n",
    "    \n",
    "    Args:\n",
    "        ruta (str): Ruta del modelo\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (modelo, tokenizador)\n",
    "    \"\"\"\n",
    "    # TODO: Implementar la carga del modelo personalizado\n",
    "    # ...\n",
    "    \n",
    "    return modelo, tokenizador\n",
    "\n",
    "# Interfaz web simple con Gradio\n",
    "def crear_interfaz_web(chatbot):\n",
    "    \"\"\"\n",
    "    Crea una interfaz web simple para el chatbot usando Gradio.\n",
    "    \n",
    "    Args:\n",
    "        chatbot: Instancia del chatbot\n",
    "        \n",
    "    Returns:\n",
    "        gr.Interface: Interfaz de Gradio\n",
    "    \"\"\"\n",
    "    # TODO: Implementar la interfaz con Gradio\n",
    "    # Definir función de callback para procesar entradas y generar respuestas\n",
    "    # ...\n",
    "    \n",
    "    # TODO: Configurar la interfaz con componentes adecuados\n",
    "    # ...\n",
    "    \n",
    "    return interfaz\n",
    "\n",
    "# Función principal para el despliegue\n",
    "def main_despliegue():\n",
    "    # TODO: Cargar el modelo personalizado\n",
    "    # ...\n",
    "    \n",
    "    # TODO: Crear instancia del chatbot\n",
    "    # ...\n",
    "    \n",
    "    # TODO: Crear y lanzar la interfaz web\n",
    "    # ...\n",
    "    \n",
    "    # TODO: (Opcional) Configurar parámetros para el despliegue\n",
    "    # ...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_despliegue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2b3b72",
   "metadata": {},
   "source": [
    "# Preguntas Teóricas\n",
    "## 1. ¿Cuáles son las diferencias fundamentales entre los modelos encoder-only, decoder-only y encoder-decoder en el contexto de los chatbots conversacionales? Explique qué tipo de modelo sería más adecuado para cada caso de uso y por qué."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a93284",
   "metadata": {},
   "source": [
    "| Tipo de Modelo     | Características Principales                                                                 | Casos de Uso en Chatbots                                      | Ejemplos de Modelos     |\n",
    "|--------------------|---------------------------------------------------------------------------------------------|---------------------------------------------------------------|--------------------------|\n",
    "| **Encoder-only**   | Solo codifica texto de entrada. Bidireccional. No genera texto.                            | Clasificación de intenciones, análisis de sentimientos        | BERT, RoBERTa            |\n",
    "| **Decoder-only**   | Genera texto palabra por palabra. Autoregresivo. Unidireccional.                            | Generación de respuestas, completado de texto                 | GPT, GPT-2, GPT-3        |\n",
    "| **Encoder-Decoder**| Codifica la entrada y luego genera una salida basada en esa codificación.                  | Traducción, resumen, generación controlada de respuestas      | T5, BART, mT5            |\n",
    "\n",
    "### ¿Qué tipo de modelo sería más adecuado para cada caso de uso y por qué?\n",
    "\n",
    "- **Encoder-only**: Útil para comprender lo que dice el usuario, como en la clasificación de intenciones o detección de entidades. No sirve para generar respuestas por sí solo.\n",
    "- **Decoder-only**: Ideal para generar texto fluido y coherente, como respuestas en un chatbot. Usado cuando la prioridad es la generación del lenguaje.\n",
    "- **Encoder-Decoder**: Útil cuando se necesita transformar texto de una forma a otra, como en chatbots que resumen información, traducen, o responden con base en entradas complejas.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6bab77",
   "metadata": {},
   "source": [
    "## 2. Explique el concepto de \"temperatura\" en la generación de texto con LLMs. ¿Cómo afecta al comportamiento del chatbot y qué consideraciones debemos tener al ajustar este parámetro para diferentes aplicaciones?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea589cc",
   "metadata": {},
   "source": [
    "## ¿Qué es la Temperatura en la Generación de Texto con LLMs?\n",
    "\n",
    "La **temperatura** es un hiperparámetro que se utiliza durante la generación de texto con modelos de lenguaje como GPT para controlar la **aleatoriedad** de las predicciones.\n",
    "\n",
    "### ¿Cómo Funciona?\n",
    "\n",
    "Durante la generación, el modelo calcula una distribución de probabilidad sobre el siguiente token (palabra o subpalabra). La temperatura modifica esta distribución:\n",
    "\n",
    "- Se aplica una fórmula como esta:  \n",
    "P_i = exp(log(P_i) / T) / sum_j(exp(log(P_j) / T))\n",
    "\n",
    "Donde `T` es la temperatura.\n",
    "\n",
    "### Efecto de la Temperatura\n",
    "\n",
    "- **Temperatura = 1.0** (valor por defecto):  \n",
    "Comportamiento **estándar** del modelo.\n",
    "\n",
    "- **Temperatura < 1.0** (por ejemplo, 0.2 a 0.7):  \n",
    "La distribución se **agudiza** → el modelo es **más conservador y repetitivo**.  \n",
    "Útil cuando se necesita **respuestas precisas, coherentes y controladas**.\n",
    "\n",
    "- **Temperatura > 1.0** (por ejemplo, 1.2 o más):  \n",
    "La distribución se **aplana** → el modelo es **más creativo, impredecible y variado**, pero puede volverse incoherente.  \n",
    "Útil para tareas como escritura creativa o brainstorming.\n",
    "\n",
    "### Ejemplos de Aplicación\n",
    "\n",
    "| Aplicación                     | Temperatura Recomendada | Justificación                                               |\n",
    "|-------------------------------|--------------------------|-------------------------------------------------------------|\n",
    "| Chatbot para atención médica  | 0.3 - 0.5                | Respuestas seguras, formales, coherentes                    |\n",
    "| Asistente educativo            | 0.5 - 0.7                | Cierta flexibilidad, pero con precisión en las explicaciones|\n",
    "| Generador de historias        | 1.0 - 1.3                | Mayor creatividad y diversidad de narrativas                |\n",
    "| Chat informal o roleplay      | 0.8 - 1.2                | Conversaciones variadas, espontáneas y menos rígidas        |\n",
    "\n",
    "### Consideraciones al Ajustar la Temperatura\n",
    "\n",
    "- No existe un valor único ideal: **depende del contexto de uso**.\n",
    "- Temperaturas muy altas pueden generar incoherencias o errores.\n",
    "- Se puede combinar con otros parámetros como `top_k` o `top_p` para refinar aún más el control del texto generado.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusión:**  \n",
    "La temperatura es clave para modular el comportamiento del chatbot. Ajustarla correctamente puede marcar la diferencia entre una respuesta aburrida y repetitiva o una conversación natural y efectiva.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14481232",
   "metadata": {},
   "source": [
    "## 3. Describa las técnicas principales para reducir el problema de \"alucinaciones\" en chatbots basados en LLMs. ¿Qué estrategias podemos implementar a nivel de inferencia y a nivel de prompt engineering para mejorar la precisión factual de las respuestas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704466d4",
   "metadata": {},
   "source": [
    "## Reducción de Alucinaciones en Chatbots Basados en LLMs\n",
    "\n",
    "Las **alucinaciones** son respuestas generadas por modelos de lenguaje que parecen correctas pero que son **fácticamente incorrectas o inventadas**. Este es uno de los desafíos principales en el uso de LLMs en aplicaciones críticas como educación, medicina o derecho.\n",
    "\n",
    "---\n",
    "\n",
    "### Técnicas para Reducir Alucinaciones\n",
    "\n",
    "#### 🧠 A. A Nivel de Inferencia\n",
    "\n",
    "1. **Temperatura baja**\n",
    "   - Usar valores de temperatura entre `0.2` y `0.5` reduce la creatividad del modelo, promoviendo respuestas más seguras y precisas.\n",
    "\n",
    "2. **Restricción de tokens (top-k / top-p sampling)**\n",
    "   - Limitar la selección de tokens a los más probables (`top_k`) o acumulando una probabilidad límite (`top_p`) ayuda a evitar resultados improbables o aleatorios.\n",
    "\n",
    "3. **Consulta iterativa y verificación**\n",
    "   - Generar respuestas en múltiples pasos o pedir al modelo que **verifique su respuesta** antes de responder definitivamente.\n",
    "\n",
    "4. **Recuperación basada en documentos (RAG)**\n",
    "   - Combinar el LLM con un sistema de recuperación de documentos externos (como una base de datos o motor de búsqueda) para responder con información verificada.\n",
    "\n",
    "---\n",
    "\n",
    "#### ✍️ B. A Nivel de Prompt Engineering\n",
    "\n",
    "1. **Instrucciones claras y específicas**\n",
    "   - Instrucciones como _“Responde solo si estás seguro”_ o _“Si no sabes la respuesta, responde ‘No lo sé’”_ pueden reducir invenciones.\n",
    "\n",
    "2. **Few-shot prompting con ejemplos reales**\n",
    "   - Incluir ejemplos correctos y bien estructurados en el prompt para establecer un patrón deseado.\n",
    "\n",
    "3. **Uso de cadenas de pensamiento (\"chain-of-thought\")**\n",
    "   - Pedir al modelo que **razone paso a paso** puede mejorar la precisión lógica y reducir errores fácticos.\n",
    "\n",
    "4. **Reforzar con roles o contexto de confiabilidad**\n",
    "   - Frases como _“Eres un experto en medicina. Responde con hechos basados en evidencia científica”_ pueden guiar al modelo hacia mayor rigor.\n",
    "\n",
    "5. **Incorporación de fuentes o referencias**\n",
    "   - Pedir al modelo que cite una fuente o diga de dónde proviene la información:  \n",
    "     _“Incluye la fuente de cada dato que menciones”_.\n",
    "\n",
    "---\n",
    "\n",
    "### Ejemplo de Prompt Mejorado\n",
    "\n",
    "```plaintext\n",
    "Eres un asistente legal que solo responde con información verificada. Si no tienes certeza, responde: \"No tengo suficiente información para responder con precisión\".\n",
    "\n",
    "Pregunta: ¿Cuál es la edad mínima para votar en Argentina?\n",
    "\n",
    "Conclusión\n",
    "Reducir las alucinaciones en LLMs requiere una combinación de ajustes técnicos e ingeniería de prompts. No existe una solución única, pero aplicar varias de estas estrategias en conjunto puede mejorar considerablemente la precisión factual del chatbot.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
